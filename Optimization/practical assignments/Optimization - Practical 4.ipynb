{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvpeGViR3EC3"
   },
   "source": [
    "Theodoros lambrou\n",
    "\n",
    "Constrained Optimization: Equality Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvjTwgxT3YYC"
   },
   "source": [
    "1. Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L20RRALP33Nd"
   },
   "source": [
    "Firstly, I define the function $f$ and the equality constraint $h$, and then compute the gradient and hessian. I also compute the langrangian as $L(x, λ) = f(x) − λh(x)$ along its gradient and hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1699808009859,
     "user": {
      "displayName": "Adriana Alvaro",
      "userId": "15456955937338907889"
     },
     "user_tz": -60
    },
    "id": "4dEaSiWS27PL"
   },
   "outputs": [],
   "source": [
    "from math import exp as e\n",
    "import numpy as np\n",
    "\n",
    "def f(x_1,x_2):\n",
    "  return e(3*x_1) + e(-4*x_2)\n",
    "\n",
    "def h(x_1,x_2):\n",
    "  return x_1**2 + x_2**2 - 1\n",
    "\n",
    "\n",
    "def grad_f(x_1, x_2):\n",
    "    return np.array([3*e(3*x_1), -4*e(-4*x_2)])\n",
    "\n",
    "def grad_h(x_1, x_2):\n",
    "    return np.array([2*x_1, 2*x_2])\n",
    "\n",
    "def hess_f(x_1, x_2):\n",
    "    return np.array([[9*e(3*x_1), 0], [0, 16*e(-4*x_2)]])\n",
    "\n",
    "def hess_h(x_1, x_2):\n",
    "    return np.array([[2, 0], [0, 2]])\n",
    "    \n",
    "\n",
    "def lag(x_1, x_2, lamda):\n",
    "    return f(x_1, x_2) - lamda*h(x_1, x_2)\n",
    "\n",
    "def grad_lag(x_1, x_2, lamda):\n",
    "    return grad_f(x_1, x_2) - lamda*grad_h(x_1, x_2)\n",
    "\n",
    "def hess_lag(x_1, x_2, lamda):\n",
    "    return hess_f(x_1, x_2) - lamda*hess_h(x_1, x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rERN8vFiW9qx"
   },
   "source": [
    "Then I define the Newtons based iterative method to find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1699808010534,
     "user": {
      "displayName": "Adriana Alvaro",
      "userId": "15456955937338907889"
     },
     "user_tz": -60
    },
    "id": "ArEiS1zlW-GI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial point x0 = [-1, 1]\n",
      "Iterations: 0\n",
      "x = (x_1, x_2) = (-0.77423, 0.72577), lamda = -0.351038\n",
      "Iterations: 1\n",
      "x = (x_1, x_2) = (-0.74865, 0.66614), lamda = -0.216059\n",
      "Break by Lagrangian Gradient\n",
      "Final result: (-0.7483381762503777, 0.663323446868971, -0.21232390186241443)\n"
     ]
    }
   ],
   "source": [
    "def solveNewtonBased(x_1, x_2, lamda, alpha=1, eps=1e-5, MAX_ITER=100, verbose=False):\n",
    "    \n",
    "    print(f'Initial point x0 = [{x_1}, {x_2}]')\n",
    "    \n",
    "    for i in range(MAX_ITER):\n",
    "\n",
    "        gh = grad_h(x_1, x_2)\n",
    "        grad_lag_value = grad_lag(x_1, x_2, lamda)\n",
    "        hess_lag_value = hess_lag(x_1, x_2, lamda)\n",
    "\n",
    "        A = np.block([[hess_lag_value, -gh.reshape(-1, 1)], [-gh, np.array([[0]])]])\n",
    "        b = np.concatenate([-grad_lag_value, [h(x_1, x_2)]])\n",
    "\n",
    "        delta = np.linalg.solve(A, b) #solving the linear system\n",
    "\n",
    "        x_1 += alpha * delta[0]\n",
    "        x_2 += alpha * delta[1]\n",
    "        lamda += alpha * delta[2]\n",
    "\n",
    "        if np.linalg.norm(grad_lag(x_1, x_2, lamda)) < eps:\n",
    "          if verbose: print('Break by Lagrangian Gradient')\n",
    "          break\n",
    "\n",
    "        if verbose:\n",
    "          print('Iterations: {}'.format(i))\n",
    "          print('x = (x_1, x_2) = ({0:.5f}, {1:.5f}), lamda = {2:.6f}'.format(x_1, x_2, lamda))\n",
    "\n",
    "    return x_1,x_2, lamda\n",
    "\n",
    "\n",
    "x_1, x_2, lamda = -1, 1, -1\n",
    "\n",
    "result = solveNewtonBased(x_1, x_2, lamda, alpha=1, eps=1e-5, MAX_ITER=100, verbose=True)\n",
    "\n",
    "print(\"Final result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f7-XFSh7cLL"
   },
   "source": [
    "It can be seen that the solution is reached in 2 iterations (and it matches the given solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERCOLDDJ8t7A"
   },
   "source": [
    "2. Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol5OISX_87GD"
   },
   "source": [
    "I repeat the previous method after I define some points which are further away of the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1699808010535,
     "user": {
      "displayName": "Adriana Alvaro",
      "userId": "15456955937338907889"
     },
     "user_tz": -60
    },
    "id": "v-o9gX6t9OJb",
    "outputId": "c7665bf9-6bf9-43ea-b957-a66b1af5f986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.23081254 -6.92163864 -7.10536778]\n",
      " [ 6.77687368  5.56285214  6.00589472]]\n"
     ]
    }
   ],
   "source": [
    "ranges = [\n",
    "    (-8, -4),  \n",
    "    (3, 7),  \n",
    "]\n",
    "\n",
    "points = np.array([\n",
    "    np.random.uniform(low, high, 3) for low, high in ranges\n",
    "])\n",
    "\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point:\n",
      "x = (x_1, x_2) = (-7.59140, -5.68915), lamda = -8.62714\n",
      "Initial point x0 = [-7.591397202007906, -5.689151444425505]\n",
      "Iterations: 0\n",
      "x = (x_1, x_2) = (-1.91713, -5.43915), lamda = -6.448436\n",
      "Iterations: 1\n",
      "x = (x_1, x_2) = (5.78713, -5.18915), lamda = -25.973879\n",
      "Iterations: 2\n",
      "x = (x_1, x_2) = (0.94601, -4.86291), lamda = -121524887.606792\n",
      "Iterations: 3\n",
      "x = (x_1, x_2) = (-3.81388, -3.36823), lamda = -611457156.528497\n",
      "Iterations: 4\n",
      "x = (x_1, x_2) = (-1.97432, -1.75627), lamda = -294926428.737318\n",
      "Iterations: 5\n",
      "x = (x_1, x_2) = (-1.12853, -1.00391), lamda = -126345116.255457\n",
      "Iterations: 6\n",
      "x = (x_1, x_2) = (-0.81160, -0.72197), lamda = -35482395.040886\n",
      "Iterations: 7\n",
      "x = (x_1, x_2) = (-0.74971, -0.66692), lamda = -2705417.899128\n",
      "Iterations: 8\n",
      "x = (x_1, x_2) = (-0.74717, -0.66465), lamda = -9197.180354\n",
      "Iterations: 9\n",
      "x = (x_1, x_2) = (-0.74870, -0.66291), lamda = 18.672930\n",
      "Iterations: 10\n",
      "x = (x_1, x_2) = (-1.06219, -0.30885), lamda = -7.831214\n",
      "Iterations: 11\n",
      "x = (x_1, x_2) = (-1.01457, -0.11058), lamda = -0.417786\n",
      "Iterations: 12\n",
      "x = (x_1, x_2) = (-1.02050, 0.13187), lamda = -0.066761\n",
      "Iterations: 13\n",
      "x = (x_1, x_2) = (-0.96013, 0.37604), lamda = -0.085227\n",
      "Iterations: 14\n",
      "x = (x_1, x_2) = (-0.84392, 0.58865), lamda = -0.128543\n",
      "Iterations: 15\n",
      "x = (x_1, x_2) = (-0.75030, 0.67300), lamda = -0.195296\n",
      "Iterations: 16\n",
      "x = (x_1, x_2) = (-0.74854, 0.66317), lamda = -0.212096\n",
      "Break by Lagrangian Gradient\n",
      "Starting point:\n",
      "x = (x_1, x_2) = (-2.09947, -2.74047), lamda = -1.49257\n",
      "Initial point x0 = [-2.0994736548888566, -2.7404662591639064]\n",
      "Iterations: 0\n",
      "x = (x_1, x_2) = (0.17435, -2.49046), lamda = -1.626793\n",
      "Iterations: 1\n",
      "x = (x_1, x_2) = (-11.13877, -2.23189), lamda = -583.685251\n",
      "Iterations: 2\n",
      "x = (x_1, x_2) = (-5.44247, -1.97334), lamda = -298.492855\n",
      "Iterations: 3\n",
      "x = (x_1, x_2) = (-2.54998, -1.71237), lamda = -158.638992\n",
      "Iterations: 4\n",
      "x = (x_1, x_2) = (-1.07425, -1.44712), lamda = -91.809718\n",
      "Iterations: 5\n",
      "x = (x_1, x_2) = (-0.39516, -1.17446), lamda = -58.206363\n",
      "Iterations: 6\n",
      "x = (x_1, x_2) = (-0.41025, -0.94140), lamda = 1.114355\n",
      "Iterations: 7\n",
      "x = (x_1, x_2) = (-0.92444, -0.68836), lamda = -0.817271\n",
      "Iterations: 8\n",
      "x = (x_1, x_2) = (-0.93213, -0.43947), lamda = -0.092199\n",
      "Iterations: 9\n",
      "x = (x_1, x_2) = (-1.01679, -0.18936), lamda = -0.064891\n",
      "Iterations: 10\n",
      "x = (x_1, x_2) = (-1.02902, 0.06043), lamda = -0.066495\n",
      "Iterations: 11\n",
      "x = (x_1, x_2) = (-0.98415, 0.30707), lamda = -0.078377\n",
      "Iterations: 12\n",
      "x = (x_1, x_2) = (-0.88119, 0.53472), lamda = -0.112358\n",
      "Iterations: 13\n",
      "x = (x_1, x_2) = (-0.76501, 0.66782), lamda = -0.178044\n",
      "Iterations: 14\n",
      "x = (x_1, x_2) = (-0.74810, 0.66381), lamda = -0.211521\n",
      "Break by Lagrangian Gradient\n"
     ]
    }
   ],
   "source": [
    "for point in points:\n",
    "    x_1, x_2, lamda = point\n",
    "    print('Starting point:\\nx = (x_1, x_2) = ({0:.5f}, {1:.5f}), lamda = {2:.5f}'.format(x_1, x_2, lamda))\n",
    "    solveNewtonBased(x_1, x_2, lamda, verbose=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that when the starting points are farther away from the optimal solution, the method diverges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZnUX_UD-1g6"
   },
   "source": [
    "3. Experiment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaqEcBYwATzV"
   },
   "source": [
    "I define the merit function and its gradient, and then use the gradient descent method with gradient normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1699808010536,
     "user": {
      "displayName": "Adriana Alvaro",
      "userId": "15456955937338907889"
     },
     "user_tz": -60
    },
    "id": "zXZfJlDfA7G9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation: [-0.85499654  0.4660693 ]\n"
     ]
    }
   ],
   "source": [
    "def merit(x_1, x_2, ro=10):\n",
    "    return f(x_1, x_2) + ro*h(x_1, x_2)**2\n",
    "\n",
    "def grad_merit(x_1, x_2, ro=10):\n",
    "    return grad_f(x_1, x_2) + 2 * ro * h(x_1, x_2) * grad_h(x_1, x_2)\n",
    "    \n",
    "def gradient_descent(f, grad_f, w0, f_tol=1e-3, grad_tol=1e-5):\n",
    "    x = [w0]\n",
    "    \n",
    "    while True:\n",
    "        gradient_of_f = grad_f(w0[0], w0[1])\n",
    "        grad_normalized = gradient_of_f / np.linalg.norm(gradient_of_f)\n",
    "        alpha = 1\n",
    "\n",
    "        while f(*(w0 - alpha * grad_normalized)) >= f(*w0): alpha /= 2\n",
    "\n",
    "        w0 = w0 - alpha * grad_normalized\n",
    "        x.append(w0)\n",
    "\n",
    "        gradient_of_f = grad_f(w0[0], w0[1])\n",
    "        grad_normalized = gradient_of_f / np.linalg.norm(gradient_of_f)\n",
    "       \n",
    "        if np.abs(f(*x[-1]) - f(*x[-2])) < f_tol or np.linalg.norm(grad_normalized) < grad_tol: return np.array(x)\n",
    "\n",
    "\n",
    "#testing the method by selecting a far away point \n",
    "point=points[0]\n",
    "solution_approx= gradient_descent(merit, grad_merit, point[:2])\n",
    "print(\"Approximation:\", solution_approx[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-5atKe7CSce"
   },
   "source": [
    "Using gradient descent method (and gradient normalization) on merit function, we get the approximation above.\n",
    "\n",
    "So it can be seen that we get closer to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVmdvRJWCecD"
   },
   "source": [
    "4. Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEQ-BRvLDEUD"
   },
   "source": [
    "I now use the point that is closer to the optimal solution and minimize the merit function with gradient descent. I then apply the Newton-based method to find the optimal solution by using the minimizer point for the merit function as the starting point for the Newton-based method. Hence, I take advanrage of the Newton-based alogrithm without having the problem of not performing well when the starting point is away from the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1699808015283,
     "user": {
      "displayName": "Adriana Alvaro",
      "userId": "15456955937338907889"
     },
     "user_tz": -60
    },
    "id": "aeCoFKxMExKP",
    "outputId": "1a3b239d-b6d9-4cad-d18a-65642df84d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial point x0 = [-0.8549965437637832, 0.46606930041339356]\n",
      "Iterations: 0\n",
      "x = (x_1, x_2) = (-0.82965, 0.56809), lamda = -0.174847\n",
      "Iterations: 1\n",
      "x = (x_1, x_2) = (-0.75848, 0.66231), lamda = -0.197098\n",
      "Iterations: 2\n",
      "x = (x_1, x_2) = (-0.74823, 0.66352), lamda = -0.212118\n",
      "Break by Lagrangian Gradient\n"
     ]
    }
   ],
   "source": [
    "solution_x =solveNewtonBased(solution_approx[-1,0], solution_approx[-1,1], lamda=-1, alpha=1, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
