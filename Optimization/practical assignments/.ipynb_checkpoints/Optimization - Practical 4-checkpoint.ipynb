{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lab4: 06/11/2023\n","## Equality Constraints\n","### Dafni Tziakouri\n","### Adriana Álvaro\n"],"metadata":{"id":"KvpeGViR3EC3"}},{"cell_type":"markdown","source":["## 2 Sequential Quadratic Programming\n"],"metadata":{"id":"dj6Fq-ld3kIK"}},{"cell_type":"markdown","source":["### **1.** One simple way to proceed is to take $α^k = 1$ and iteratively update the current point to obtain the next. This is a simple way to proceed that is proposed to perform first. The stopping condition should be performed over $∇_xL$. Test this approach and check if it works using the starting point proposed in the example."],"metadata":{"id":"yvjTwgxT3YYC"}},{"cell_type":"markdown","source":["We will start by defining the function $f$ and the equality constraint $h$:"],"metadata":{"id":"L20RRALP33Nd"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"4dEaSiWS27PL","executionInfo":{"status":"ok","timestamp":1699808009859,"user_tz":-60,"elapsed":222,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}}},"outputs":[],"source":["from math import exp as e\n","import numpy as np\n","\n","def f(x_1,x_2):\n","  return e(3*x_1) + e(-4*x_2)\n","\n","def h(x_1,x_2):\n","  return x_1**2 + x_2**2 - 1"]},{"cell_type":"markdown","source":["Let's also compute the gradient and the hessian:"],"metadata":{"id":"hgOaDs3j4sCG"}},{"cell_type":"code","source":["def grad_f(x_1, x_2):\n","    return np.array([3*e(3*x_1), -4*e(-4*x_2)])\n","\n","def grad_h(x_1, x_2):\n","    return np.array([2*x_1, 2*x_2])\n","\n","def hess_f(x_1, x_2):\n","    return np.array([[9*e(3*x_1), 0], [0, 16*e(-4*x_2)]])\n","\n","def hess_h(x_1, x_2):\n","    return np.array([[2, 0], [0, 2]])"],"metadata":{"id":"hSe7tl3N4yMQ","executionInfo":{"status":"ok","timestamp":1699808010098,"user_tz":-60,"elapsed":6,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["We will compute the langrangian as $L(x, λ) = f(x) − λh(x)$ and also it's gradient and hessian."],"metadata":{"id":"fajl7p205EyV"}},{"cell_type":"code","source":["def lag(x_1, x_2, lamda):\n","    return f(x_1, x_2) - lamda*h(x_1, x_2)\n","\n","def grad_lag(x_1, x_2, lamda):\n","    return grad_f(x_1, x_2) - lamda*grad_h(x_1, x_2)\n","\n","def hess_lag(x_1, x_2, lamda):\n","    return hess_f(x_1, x_2) - lamda*hess_h(x_1, x_2)"],"metadata":{"id":"_PALptOY7hrd","executionInfo":{"status":"ok","timestamp":1699808010099,"user_tz":-60,"elapsed":6,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["We will define the Newtons based iterative method to solve the problem."],"metadata":{"id":"rERN8vFiW9qx"}},{"cell_type":"code","source":["def solveNewtonBased(x_1, x_2, lamda, alpha=1, eps=1e-5, MAX_ITER=100, verbose=False):\n","    print(f'Initial point x0 = [{x_1}, {x_2}]')\n","    for i in range(MAX_ITER):\n","        # Compute necessary gradients and Hessian matrices\n","        gh = grad_h(x_1, x_2)\n","        grad_lag_value = grad_lag(x_1, x_2, lamda)\n","        hess_lag_value = hess_lag(x_1, x_2, lamda)\n","\n","        # Build the matrix A and vector b\n","        A = np.block([[hess_lag_value, -gh.reshape(-1, 1)], [-gh, np.array([[0]])]])\n","        b = np.concatenate([-grad_lag_value, [h(x_1, x_2)]])\n","\n","        # Solve the linear system A * delta = b\n","        delta = np.linalg.solve(A, b)\n","\n","        # Update variables\n","        x_1 += alpha * delta[0]\n","        x_2 += alpha * delta[1]\n","        lamda += alpha * delta[2]\n","\n","        if np.linalg.norm(grad_lag(x_1, x_2, lamda)) < eps:\n","          if verbose:\n","                print('Break by Lagrangian gradient')\n","          break\n","\n","        if verbose:\n","          print('Iterations: {}'.format(i))\n","          print('x = (x_1, x_2) = ({0:.5f}, {1:.5f}), lamda = {2:.5f}'.format(x_1, x_2, lamda))\n","\n","    return x_1, x_2, lamda\n"],"metadata":{"id":"ArEiS1zlW-GI","executionInfo":{"status":"ok","timestamp":1699808010534,"user_tz":-60,"elapsed":440,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["x_1, x_2, lamda = -1, 1, -1\n","result = solveNewtonBased(x_1, x_2, lamda, alpha=1, eps=1e-5, MAX_ITER=100, verbose=True)\n","\n","print(\"Final result:\", result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ihv3i729YHkI","executionInfo":{"status":"ok","timestamp":1699808010534,"user_tz":-60,"elapsed":32,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}},"outputId":"240090d3-f6e0-4fe1-b501-3fe969cc8c95"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial point x0 = [-1, 1]\n","Iterations: 0\n","x = (x_1, x_2) = (-0.77423, 0.72577), lamda = -0.35104\n","Iterations: 1\n","x = (x_1, x_2) = (-0.74865, 0.66614), lamda = -0.21606\n","Break by Lagrangian gradient\n","Final result: (-0.7483381762503777, 0.663323446868971, -0.21232390186241443)\n"]}]},{"cell_type":"markdown","source":["We observe that the solution is reached in two iterations and it also matches the given solution in the PDF file."],"metadata":{"id":"-f7-XFSh7cLL"}},{"cell_type":"markdown","source":["### **2.** This basic iteration also has drawbacks, leading to a number of vital questions. It is a Newtonlike iteration, and thus may diverge from poor starting points. In our example we have started from a point that is near to the optimal solution. Try to perform some experiments with starting points that are farther away of the optimal solution."],"metadata":{"id":"ERCOLDDJ8t7A"}},{"cell_type":"markdown","source":["We will define some points which are farther away of the optimal solution and try again the previous method."],"metadata":{"id":"ol5OISX_87GD"}},{"cell_type":"code","source":["# Define larger ranges\n","farther_ranges = [\n","    (-10, -5),  # Increase the range for component 1\n","    (-4, -1),  # Increase the range for component 2\n","    (3, 6),  # Increase the range for component 3\n","]\n","\n","far_away_points = np.array([\n","    np.random.uniform(low, high, 3) for low, high in farther_ranges\n","])\n","\n","print(far_away_points)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-o9gX6t9OJb","executionInfo":{"status":"ok","timestamp":1699808010535,"user_tz":-60,"elapsed":28,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}},"outputId":"c7665bf9-6bf9-43ea-b957-a66b1af5f986"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-8.2721592  -5.03411667 -8.7863411 ]\n"," [-2.58883276 -2.3796944  -1.56814779]\n"," [ 4.07181224  5.1065274   4.79671312]]\n"]}]},{"cell_type":"code","source":["for point in far_away_points:\n","    x_1, x_2, lamda = point\n","    print('Starting point:\\nx = (x_1, x_2) = ({0:.5f}, {1:.5f}), lamda = {2:.5f}'.format(x_1, x_2, lamda))\n","    solveNewtonBased(x_1, x_2, lamda, verbose=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJSbq6_B-Ld2","executionInfo":{"status":"ok","timestamp":1699808010535,"user_tz":-60,"elapsed":23,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}},"outputId":"b223b6a7-f099-4735-a184-35569d6b05b6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting point:\n","x = (x_1, x_2) = (-8.27216, -5.03412), lamda = -8.78634\n","Initial point x0 = [-8.272159202617534, -5.034116665029588]\n","Iterations: 0\n","x = (x_1, x_2) = (-2.81688, -4.78412), lamda = -5.79437\n","Iterations: 1\n","x = (x_1, x_2) = (2.05208, -4.53412), lamda = -10.01730\n","Iterations: 2\n","x = (x_1, x_2) = (-3.18696, -4.28408), lamda = -5099.42339\n","Iterations: 3\n","x = (x_1, x_2) = (0.79285, -4.03396), lamda = -6368.04871\n","Iterations: 4\n","x = (x_1, x_2) = (-7.94552, -3.78048), lamda = -70700.19405\n","Iterations: 5\n","x = (x_1, x_2) = (-3.25751, -3.52575), lamda = -41714.45614\n","Iterations: 6\n","x = (x_1, x_2) = (-0.15789, -3.26365), lamda = -39692.58779\n","Iterations: 7\n","x = (x_1, x_2) = (4.49251, -2.00619), lamda = -1169175.90962\n","Iterations: 8\n","x = (x_1, x_2) = (2.89892, 0.20921), lamda = -1315048.14894\n","Iterations: 9\n","x = (x_1, x_2) = (1.62114, 0.11560), lamda = -588414.28842\n","Iterations: 10\n","x = (x_1, x_2) = (1.11744, 0.07967), lamda = -182888.13108\n","Iterations: 11\n","x = (x_1, x_2) = (1.00391, 0.07160), lamda = -18555.91235\n","Iterations: 12\n","x = (x_1, x_2) = (0.99747, 0.07133), lamda = -89.13054\n","Iterations: 13\n","x = (x_1, x_2) = (0.99474, 0.10924), lamda = 29.48674\n","Iterations: 14\n","x = (x_1, x_2) = (1.01516, -0.08329), lamda = 31.03451\n","Iterations: 15\n","x = (x_1, x_2) = (0.99543, -0.09872), lamda = 29.82408\n","Iterations: 16\n","x = (x_1, x_2) = (0.99498, -0.10005), lamda = 29.82791\n","Break by Lagrangian gradient\n","Starting point:\n","x = (x_1, x_2) = (-2.58883, -2.37969), lamda = -1.56815\n","Initial point x0 = [-2.588832758683221, -2.3796944000914833]\n","Iterations: 0\n","x = (x_1, x_2) = (-0.62365, -2.12967), lamda = -1.19207\n","Iterations: 1\n","x = (x_1, x_2) = (1.66768, -1.87929), lamda = -7.29580\n","Iterations: 2\n","x = (x_1, x_2) = (0.41053, -1.58134), lamda = -376.58593\n","Iterations: 3\n","x = (x_1, x_2) = (-0.11688, -1.19049), lamda = -491.09414\n","Iterations: 4\n","x = (x_1, x_2) = (-0.11427, -1.00976), lamda = -20.10596\n","Iterations: 5\n","x = (x_1, x_2) = (-0.55838, -0.94333), lamda = 81.24200\n","Iterations: 6\n","x = (x_1, x_2) = (-3.21711, 0.73734), lamda = -383.32308\n","Iterations: 7\n","x = (x_1, x_2) = (-1.75608, 0.40312), lamda = -174.08405\n","Iterations: 8\n","x = (x_1, x_2) = (-1.14775, 0.26698), lamda = -60.31773\n","Iterations: 9\n","x = (x_1, x_2) = (-0.98443, 0.24129), lamda = -8.64476\n","Iterations: 10\n","x = (x_1, x_2) = (-0.95627, 0.29956), lamda = -0.33353\n","Iterations: 11\n","x = (x_1, x_2) = (-0.88971, 0.50503), lamda = -0.13004\n","Iterations: 12\n","x = (x_1, x_2) = (-0.77850, 0.65477), lamda = -0.17210\n","Iterations: 13\n","x = (x_1, x_2) = (-0.74746, 0.66511), lamda = -0.21066\n","Iterations: 14\n","x = (x_1, x_2) = (-0.74834, 0.66332), lamda = -0.21232\n","Break by Lagrangian gradient\n","Starting point:\n","x = (x_1, x_2) = (4.07181, 5.10653), lamda = 4.79671\n","Initial point x0 = [4.071812243987658, 5.106527404952699]\n","Iterations: 0\n","x = (x_1, x_2) = (3.73849, 1.29358), lamda = 3.58162\n","Iterations: 1\n","x = (x_1, x_2) = (3.40530, -3.40594), lamda = 12.83861\n","Iterations: 2\n","x = (x_1, x_2) = (0.44518, -3.10701), lamda = -94872.52155\n","Iterations: 3\n","x = (x_1, x_2) = (-2.13399, -2.05208), lamda = -549738.89201\n","Iterations: 4\n","x = (x_1, x_2) = (-1.17161, -1.16090), lamda = -247918.55309\n","Iterations: 5\n","x = (x_1, x_2) = (-0.80095, -0.79402), lamda = -78433.77735\n","Iterations: 6\n","x = (x_1, x_2) = (-0.71552, -0.70892), lamda = -8366.20416\n","Iterations: 7\n","x = (x_1, x_2) = (-0.71238, -0.70183), lamda = -36.93486\n","Iterations: 8\n","x = (x_1, x_2) = (-0.87177, -0.54001), lamda = 8.13402\n","Iterations: 9\n","x = (x_1, x_2) = (-1.02583, -0.24353), lamda = -1.50517\n","Iterations: 10\n","x = (x_1, x_2) = (-1.02700, -0.00941), lamda = -0.06542\n","Iterations: 11\n","x = (x_1, x_2) = (-1.00259, 0.23872), lamda = -0.07353\n","Iterations: 12\n","x = (x_1, x_2) = (-0.91524, 0.47534), lamda = -0.09969\n","Iterations: 13\n","x = (x_1, x_2) = (-0.79079, 0.64805), lamda = -0.15806\n","Iterations: 14\n","x = (x_1, x_2) = (-0.74674, 0.66683), lamda = -0.20908\n","Iterations: 15\n","x = (x_1, x_2) = (-0.74836, 0.66330), lamda = -0.21231\n","Break by Lagrangian gradient\n"]}]},{"cell_type":"markdown","source":["We notice that with the selected starting point which are farter away from the optimal solution, the method diverges and is not finding the optimal solution."],"metadata":{"id":"rh7XX-hP-ko1"}},{"cell_type":"markdown","source":["### **3.** One way to find the optimal solution from points that are far away of the optimal solution is to start the optimization with another function that allows us to find an approximation to the solution we are looking for. Once an approximate solution is found, we can apply the Newton-based technique we have presented previously to find the optimal solution.\n","\n","### The function that allows us to find an approximation to the solution we are looking for is called, in this context, the merit function. Usually, a merit function is the sum of terms that include the objective function and the amount of infeasibility of the constraints. One example of a merit function for the problem we are treating is the quadratic penalty function $M(x_1, x_2) = f(x_1, x_2) + ρh(x_1, x_2)^2$ where $ρ$ is some positive number. The greater the value of ρ, the greater the penalty for infeasibility. The difficulty arises in defining a proper merit function for a particular equality constrained problem.\n","\n","###Here we propose you to take $ρ = 10$ and perform a classical gradient descent (with backtraking if you want) to find and approximation to the solution we are looking for. Observe if you arrive near to the optimal solution of the problem.Take into account that you may have numerical problems with the gradient. A simple way to deal with it is to normalize the gradient at each iteration, $∇M(x)/ ||∇M(x)||$, and use this normalized gradient as search direction.\n"],"metadata":{"id":"0ZnUX_UD-1g6"}},{"cell_type":"markdown","source":["We will define the merit function and it is gradient:"],"metadata":{"id":"gaqEcBYwATzV"}},{"cell_type":"code","source":["def merit(x_1, x_2, ro=10):\n","    return f(x_1, x_2) + ro*h(x_1, x_2)**2\n","\n","def grad_merit(x_1, x_2, ro=10):\n","    return grad_f(x_1, x_2) + 2 * ro * h(x_1, x_2) * grad_h(x_1, x_2)"],"metadata":{"id":"LwQTPHjjAkya","executionInfo":{"status":"ok","timestamp":1699808010536,"user_tz":-60,"elapsed":18,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Now, we will use the gradient descent method as it was suggested but with gradient normalization."],"metadata":{"id":"pTwIHEoRAwsl"}},{"cell_type":"code","source":["def gradient_descent(f, grad_f, w0, f_tol=1e-3, grad_tol=1e-5):\n","    x = [w0]\n","    # Iterating until one of the stop criteria is fulfilled\n","    while True:\n","        gradient_of_f = grad_f(w0[0], w0[1])\n","        grad_normalized = gradient_of_f / np.linalg.norm(gradient_of_f)\n","        alpha = 1\n","        # Compute new alphas until the needed condition is true.\n","        while f(*(w0 - alpha * grad_normalized)) >= f(*w0):\n","            alpha /= 2\n","\n","        # Same formula to compute the next iteration point\n","        w0 = w0 - alpha * grad_normalized\n","        x.append(w0)\n","\n","        # If one of the stopping criterion is satisfied, we return the history of points\n","        gradient_of_f = grad_f(w0[0], w0[1])\n","        grad_normalized = gradient_of_f / np.linalg.norm(gradient_of_f)\n","        if np.abs(f(*x[-1]) - f(*x[-2])) < f_tol or np.linalg.norm(grad_normalized) < grad_tol:\n","            return np.array(x)"],"metadata":{"id":"zXZfJlDfA7G9","executionInfo":{"status":"ok","timestamp":1699808010536,"user_tz":-60,"elapsed":17,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Let's test the method by choosing a far away point from the optimal solution."],"metadata":{"id":"QfWb1qwNBbAB"}},{"cell_type":"code","source":["point= far_away_points[0]\n","solution_approx = gradient_descent(merit, grad_merit, point[:2])\n","print(\"With a gradient descent method (and gradient normalization) on Merit function we get the approximation:\", solution_approx[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UviXYSIyBnXs","executionInfo":{"status":"ok","timestamp":1699808010537,"user_tz":-60,"elapsed":17,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}},"outputId":"683d8b1d-a878-4917-e5c2-94e137071454"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["With a gradient descent method (and gradient normalization) on Merit function we get the approximation: [-0.82151982  0.56654271]\n"]}]},{"cell_type":"markdown","source":["We notice that we get a little bit closer to the optimal solutuion but still not that close. The minimizers for the merit function do not coincide with the minimizers for the objective function."],"metadata":{"id":"p-5atKe7CSce"}},{"cell_type":"markdown","source":["### **4.** As previously commented, the minimizers of the merit function $M(x_1,x_2)$ do not necessarily have to coincide with the minimizers of the constrained problem. Thus, once we “sufficiently” approach the optimal solution we may use the Newton method (with $α = 1$) to find the solution to the problem.\n","\n","### Therefore the algorithm consists in starting with the Merit function to obtain an approximation to the optimal point we are looking for. Once an approximation to the solution is found,use the Newton-based method to find the optimal solution. Check if you are able to find the optimal solution to the problem."],"metadata":{"id":"YVmdvRJWCecD"}},{"cell_type":"markdown","source":["We will use the point which is closer to the optimal solution and minimize the merit function with gradient descent. Then, we will apply the Newton based method to find the optimal solution, using the minimizer point for the merit function as the starting point for the newton based method.\n","\n","This way, we take leverage of the newton based algorithm and we avoid the inconvenient that it had, that is not performing well when the starting point is not already close to the minimum."],"metadata":{"id":"jEQ-BRvLDEUD"}},{"cell_type":"code","source":["solution_x = solveNewtonBased(solution_approx[-1,0], solution_approx[-1,1], lamda=-1, alpha=1, verbose=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeCoFKxMExKP","executionInfo":{"status":"ok","timestamp":1699808015283,"user_tz":-60,"elapsed":7,"user":{"displayName":"Adriana Alvaro","userId":"15456955937338907889"}},"outputId":"1a3b239d-b6d9-4cad-d18a-65642df84d72"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial point x0 = [-0.8215198215944418, 0.5665427128173111]\n","Iterations: 0\n","x = (x_1, x_2) = (-0.79025, 0.61553), lamda = -0.20791\n","Iterations: 1\n","x = (x_1, x_2) = (-0.75115, 0.66299), lamda = -0.20839\n","Iterations: 2\n","x = (x_1, x_2) = (-0.74833, 0.66334), lamda = -0.21231\n","Break by Lagrangian gradient\n"]}]},{"cell_type":"markdown","source":["In this way we obtain the optimal solution given also in the PDF file."],"metadata":{"id":"_asUsLgQFEn7"}}]}